{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dietary-custody",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "from utils.load_data import *\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "BCIC_dataset = load_BCIC(\n",
    "train_sub=[1,2,3,4,5,6,7,8],\n",
    "test_sub=[9],\n",
    "alg_name = 'Tensor_CSPNet',\n",
    "scenario = 'raw-signal-si'\n",
    ")\n",
    "\n",
    "train_x, train_y, test_x, test_y = BCIC_dataset.generate_training_valid_test_set_subject_independent()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "found-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stft_datset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.X = x\n",
    "        self.y = y\n",
    "        self.stft = []\n",
    "        for sig in self.X:\n",
    "            f, t, Sxx = signal.stft(sig, fs=250)\n",
    "            self.stft.append(np.abs(Sxx))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.stft[idx], dtype=torch.float32), self.y[idx]\n",
    "    \n",
    "train_stft = Stft_datset(train_x, train_y)\n",
    "test_stft = Stft_datset(test_x, test_y)\n",
    "\n",
    "train_dataloader = DataLoader(train_stft, batch_size=32, num_workers=16)\n",
    "test_dataloader = DataLoader(test_stft, batch_size=32, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "popular-prevention",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junyeobe/.local/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from lib.sdes import VariancePreservingSDE, PluginReverseSDE\n",
    "from lib.plotting import get_grid\n",
    "from lib.flows.elemwise import LogitTransform\n",
    "from lib.helpers import logging, create\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
    "    def __init__(self, embed_dim, scale=30.):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights during initialization. These weights are fixed \n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "        \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "        Args:\n",
    "          marginal_prob_std: A function that takes time t and gives the standard\n",
    "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "          channels: The number of channels for feature maps of each resolution.\n",
    "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Gaussian random feature embedding layer for time\n",
    "        self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
    "             nn.Linear(embed_dim, embed_dim))\n",
    "        # Encoding layers where the resolution decreases\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels=1, out_channels=32, kernel_size=(10,100,3), padding=(0,0,0), bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        self.conv2 = torch.nn.Conv3d(in_channels=32, out_channels=64, kernel_size=(10,30,3), padding=(0,0,0), bias=False)\n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.conv3 = torch.nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(3,1,2), padding=(0,0,0), bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.conv4 = torch.nn.Conv3d(in_channels=128, out_channels=256, kernel_size=(2,1,11), padding=(0,0,0), bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])    \n",
    "\n",
    "        # Decoding layers where the resolution increases\n",
    "        self.tconv4 = torch.nn.ConvTranspose3d(in_channels=256, out_channels=128, kernel_size= (2,1,11), padding=(0,0,0), bias=False)\n",
    "        self.dense5 = Dense(embed_dim, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.tconv3 = torch.nn.ConvTranspose3d(in_channels=256, out_channels=64, kernel_size= (3,1,2), padding=(0,0,0), bias=False)    \n",
    "        self.dense6 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.tconv2 = torch.nn.ConvTranspose3d(in_channels=128, out_channels=32, kernel_size= (10,30,3), padding=(0,0,0), bias=False)   \n",
    "        self.dense7 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        self.tconv1 = torch.nn.ConvTranspose3d(in_channels=64, out_channels=1, kernel_size= (10,100,3), padding=(0,0,0), bias=False)\n",
    "\n",
    "        # The swish activation function\n",
    "        self.act = lambda x: x * torch.sigmoid(x)\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "    def forward(self, x, t): \n",
    "        # Obtain the Gaussian random feature embedding for t   \n",
    "        embed = self.act(self.embed(t))    \n",
    "        # Encoding path\n",
    "        h1 = self.conv1(x)    \n",
    "        ## Incorporate information from t\n",
    "        h1 += self.dense1(embed)\n",
    "        ## Group normalization\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "        # print(h1.shape)\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "        # print(h2.shape)\n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "        # print(h3.shape)\n",
    "        h4 = self.conv4(h3)\n",
    "        h4 += self.dense4(embed)\n",
    "        h4 = self.gnorm4(h4)\n",
    "        h4 = self.act(h4)\n",
    "        # print(h4.shape)\n",
    "        # Decoding path\n",
    "        h = self.tconv4(h4)\n",
    "        ## Skip connection from the encoding path\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm4(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "        h += self.dense6(embed)\n",
    "        h = self.tgnorm3(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "        h += self.dense7(embed)\n",
    "        h = self.tgnorm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None, None]\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "capable-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "device = 'cuda' #@param ['cuda', 'cpu'] {'type':'string'}\n",
    "\n",
    "def marginal_prob_std(t, sigma):\n",
    "    \"\"\"Compute the mean and standard deviation of $p_{0t}(x(t) | x(0))$.\n",
    "\n",
    "    Args:    \n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.  \n",
    "\n",
    "    Returns:\n",
    "    The standard deviation.\n",
    "    \"\"\"    \n",
    "    t = torch.tensor(t, device=device)\n",
    "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "    \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "    Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "    Returns:\n",
    "    The vector of diffusion coefficients.\n",
    "    \"\"\"\n",
    "    return torch.tensor(sigma**t, device=device)\n",
    "  \n",
    "def loss_fn(model, x, marginal_prob_std, eps=1e-5):\n",
    "    \"\"\"The loss function for training score-based generative models.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model instance that represents a \n",
    "      time-dependent score-based model.\n",
    "    x: A mini-batch of training data.    \n",
    "    marginal_prob_std: A function that gives the standard deviation of \n",
    "      the perturbation kernel.\n",
    "    eps: A tolerance value for numerical stability.\n",
    "    \"\"\"\n",
    "    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps  \n",
    "    z = torch.randn_like(x)\n",
    "    std = marginal_prob_std(random_t)\n",
    "    perturbed_x = x + z * std[:, None, None, None, None]\n",
    "    score = model(perturbed_x, random_t)\n",
    "    loss = torch.mean(torch.sum((score * std[:, None, None, None, None] + z)**2, dim=(1,2,3,4)))\n",
    "    return loss\n",
    "\n",
    "sigma =  0.1#@param {'type':'number'}\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alien-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ea1139d3ef40b59148f2010745048d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30664/1243131229.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()    \n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 31\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m num_items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(avg_loss \u001b[38;5;241m/\u001b[39m num_items)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "\n",
    "score_model = torch.nn.DataParallel(ScoreNet(marginal_prob_std=marginal_prob_std_fn))\n",
    "score_model = score_model.to(device)\n",
    "\n",
    "n_epochs =   1000\n",
    "\n",
    "lr=1e-4\n",
    "\n",
    "\n",
    "optimizer = Adam(score_model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.999 ** epoch)\n",
    "tqdm_epoch = tqdm.tnrange(n_epochs)\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm_epoch:\n",
    "    avg_loss = 0.\n",
    "    num_items = 0\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device).unsqueeze(1)\n",
    "        loss = loss_fn(score_model, x, marginal_prob_std_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() * x.shape[0]\n",
    "        num_items += x.shape[0]\n",
    "        losses.append(avg_loss / num_items)\n",
    "    scheduler.step()\n",
    "    # Print the averaged training loss so far.\n",
    "    tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "    # Update the checkpoint after each epoch of training.\n",
    "    # torch.save(score_model.state_dict(), 'ckpt.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-guidance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "## The error tolerance for the black-box ODE solver\n",
    "error_tolerance = 1e-5 #@param {'type': 'number'}\n",
    "def ode_sampler(score_model,\n",
    "                marginal_prob_std,\n",
    "                diffusion_coeff,\n",
    "                batch_size=64, \n",
    "                atol=error_tolerance, \n",
    "                rtol=error_tolerance, \n",
    "                device='cuda', \n",
    "                z=None,\n",
    "                eps=1e-3):\n",
    "    \"\"\"Generate samples from score-based models with black-box ODE solvers.\n",
    "\n",
    "    Args:\n",
    "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that returns the standard deviation \n",
    "      of the perturbation kernel.\n",
    "    diffusion_coeff: A function that returns the diffusion coefficient of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    atol: Tolerance of absolute errors.\n",
    "    rtol: Tolerance of relative errors.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    z: The latent code that governs the final sample. If None, we start from p_1;\n",
    "      otherwise, we start from the given z.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "    \"\"\"\n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    # Create the latent code\n",
    "    if z is None:\n",
    "        init_x = torch.randn(batch_size, 1, 22, 129, 16, device=device) \\\n",
    "          * marginal_prob_std(t)[:, None, None, None, None]\n",
    "    else:\n",
    "        init_x = z\n",
    "    \n",
    "    shape = init_x.shape\n",
    "\n",
    "    def score_eval_wrapper(sample, time_steps):\n",
    "        \"\"\"A wrapper of the score-based model for use by the ODE solver.\"\"\"\n",
    "        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))    \n",
    "        with torch.no_grad():    \n",
    "            score = score_model(sample, time_steps)\n",
    "        return score.cpu().numpy().reshape((-1,)).astype(np.float64)\n",
    "\n",
    "    def ode_func(t, x):        \n",
    "        \"\"\"The ODE function for use by the ODE solver.\"\"\"\n",
    "        time_steps = np.ones((shape[0],)) * t    \n",
    "        g = diffusion_coeff(torch.tensor(t)).cpu().numpy()\n",
    "        return  -0.5 * (g**2) * score_eval_wrapper(x, time_steps)\n",
    "\n",
    "    # Run the black-box ODE solver.\n",
    "    res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')  \n",
    "    print(f\"Number of function evaluations: {res.nfev}\")\n",
    "    x = torch.tensor(res.y[:, -1], device=device).reshape(shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "## Load the pre-trained checkpoint from disk.\n",
    "device = 'cuda' \n",
    "\n",
    "sample_batch_size = 64 \n",
    "sampler = ode_sampler\n",
    "\n",
    "## Generate samples using the specified sampler.\n",
    "samples = sampler(score_model,\n",
    "                marginal_prob_std_fn,\n",
    "                diffusion_coeff_fn,\n",
    "                batch_size=1, \n",
    "                atol=error_tolerance, \n",
    "                rtol=error_tolerance, \n",
    "                device='cuda', \n",
    "                z=None,\n",
    "                eps=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-better",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "show = samples[0][0]\n",
    "\n",
    "for plot in show:\n",
    "    plt.plot(plot.cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-portfolio",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
